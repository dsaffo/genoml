# -*- coding: utf-8 -*-
"""Prune_filter_merge_export_forTrainingDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11X_y89v_xxDLWlQNSpGQ_XA-xUYTFQVr
"""



"""## Prune SNPs, filter inputs, export \*.dataForML in HDF5
 - **Project:** GenoML
 - **Author(s):** Mary B. Makarious, Mike A. Nalls
 - **Date Notebook Started:** 21.08.2019
    - **Quick Description:** Complete rewrite of the initial GenoML data management module, also known as the first step in the pipeline.

---
### Quick Description: 
**Problem:** We need to read, filter and merge diverse data.

**Solution:** Automate it with very few options for people to mess with. 

### Motivation/Background:
Data management needs to be simple and based on "switches".   corresponding to what the input data is.  
Make the process "bomb proof".  
Most implementations of auto-ML mess up at a foundational basic stage.  

### Concerns/Impact on Related Code: 
Complete revision of previous work.  
Dropped the PRSice workflow from previous versions to simplified P filter. 
Even changes to storage (HDF5).  
Everything starts here.  
Depends on Mary M.'s VIF code.  
Make sure PLINK and GenoML are in the user PATH.  

### Thoughts for Future Development of Code in this Notebook: 
Speed it up (numba).  
PLINK2 formats for genotypes.  
Possibly re-introduce the PRSice workflow.

---
## Proposed Workflow
#### [0. Imports](#0)
 - Duh, read in the necessary packages.  
 
#### [1. Command Args](#1)
 - Read in genotype file (PLINK binary) ... PLINK prune?
 - Read in addit file (additional CSV data) ... VIF prune?  
 - Read in GWAS summary stats file (additional CSV data) with some P filter ... GWAS prune?
 - Read in phenotype file.
 - Mean or median imputation?
 - How heavy do you want your VIF and for how many iterations?

#### [2. Read in raw data](#2)
 - PLINK format genotypes. Note, if FID and IID differ, only IID will be used to match.
 - CSV addit file and pheno file. Only ID column then features or PHENO column. Should correspond to IID column in PLINK.
 - CSV GWAS summary stats.
 
#### [3. Prune with PLINK](#3)
 - Either standard PLINK prune or if GWAS is specified employ a user specified pre-filter based on a P threshold. 
 
#### [4. Impute missing data](#4)
 - Impute missing data in pruned genotype file.
 - Impute missing data in addit file.

#### [5. VIF non-genotype features](#5)
 - Prune non-genotype features.
 - Mary M. runs this, see her other notebook for draft!

#### [6. Scale continuous data](#6)
 - Get the additional non-genotype features on roughly the genotype scale via Z tranforming.  


#### [7. Export filtered subsets of data to HDF5](#7)
 - Export filtered genotypes.
 - Export fitlered additional features.

#### [8. Re-read from HDF5, merge and export merged dataset for analysis](#8)
 - Merge HDF5 versions of genotypes and/or additional features with the phenotype file. HDF5 merges faster.
 - Export *\.dataForML in HDF5.

# Proposed Workflow

--- 
<a id="0"></a>
## 0. Imports
"""

import os
import sys
import argparse
import math
import time
import h5py
import joblib
import subprocess
import numpy as np
import pandas as pd

"""--- 
<a id="1"></a>
## 1. Command args
"""

parser = argparse.ArgumentParser(description='Arguments for building a training dataset for GenoML.')    
parser.add_argument('--prefix', type=str, default='GenoML_data', help='Prefix for your training data build.')
parser.add_argument('--geno', type=str, default='nope', help='Genotype: (string file path). Path to PLINK format genotype file, everything before the *.bed/bim/fam [default: nope].')
parser.add_argument('--addit', type=str, default='nope', help='Additional: (string file path). Path to CSV format feature file [default: nope].')
parser.add_argument('--pheno', type=str, default='lost', help='Phenotype: (string file path). Path to CSV phenotype file [default: lost].')
parser.add_argument('--gwas', type=str, default='nope', help='GWAS summary stats: (string file path). Path to CSV format external GWAS summary statistics containing at least the columns SNP and P in the header [default: nope].')
parser.add_argument('--p', type=float, default=0.001, help='P threshold for GWAS: (some value between 0-1). P value to filter your SNP data on [default: 0.001].')
parser.add_argument('--vif', type=int, default=0, help='Variance Inflation Factor (VIF): (integer). This is the VIF threshold for pruning non-genotype features. We recommend a value of 5-10. The default of 0 means no VIF filtering will be done. [default: 0].')
parser.add_argument('--iter', type=int, default=0, help='Iterator: (integer). How many iterations of VIF pruning of features do you want to run. To save time VIF is run in randomly assorted chunks of 1000 features per iteration. The default of 0 means only one pass through the data. [default: 0].')
parser.add_argument('--impute', type=str, default='median', help='Imputation: (mean, median). Governs secondary imputation and data transformation [default: median].')


args = parser.parse_args()

print("")

print("Here is some basic info on the command you are about to run.")
print("Python version info...")
print(sys.version)
print("CLI argument info...")
print("The output prefix for this run is", args.prefix, "and will be appended to later runs of GenoML.")
print("Working with genotype data?", args.geno)
print("Working with additional predictors?", args.addit)
print("Where is your phenotype file?", args.pheno)
print("Any use for an external set of GWAS summary stats?", args.gwas)
print("If you plan on using external GWAs summary stats for SNP filtering, we'll only keep SNPs at what P value?", args.p)
print("How strong is your VIF filter?", args.vif)
print("How many iterations of VIF filtering are you doing?", args.iter)
print("The imputation method you picked is using the column", args.impute, "to fill in any remaining NAs.")
print("Give credit where credit is due, for this stage of analysis we use code from the great contributors to python packages: os, sys, argparse, numpy, pandas, joblib, math and time. We also use PLINKv1.9 from https://www.cog-genomics.org/plink/1.9/.")

run_prefix = args.prefix

print("")

"""--- 
<a id="2"></a>
## 2. Read in raw data
"""

pheno_path = args.pheno
if (pheno_path == "lost"):
  print("Looks like you lost your phenotype file. Just give up because you are currently don't have anything to predict.")
if (pheno_path != "lost"):
  pheno_df = pd.read_csv(pheno_path, engine = 'c')

addit_path = args.addit
if (addit_path == "nope"):
  print("No additional features as predictors? No problem, we'll stick to genotypes.")
if (addit_path != "nope"):
  addit_df = pd.read_csv(addit_path, engine = 'c')

gwas_path = args.gwas
if (gwas_path == "nope"):
  print("So you don't want to filter on P values from external GWAS? No worries, we don't usually either (if the dataset is large enough).")
if (gwas_path != "nope"):
  gwas_df = pd.read_csv(gwas_path, engine = 'c')

geno_path = args.geno
if (geno_path == "nope"):
  print("So no genotypes? Okay, we'll just use additional features provided for the predictions.")
if (geno_path != "nope"):
  print("Pruning your data and exporting a reduced set of genotypes.")

"""--- 
<a id="3"></a>
## 3. Prune with PLINK
"""

# Set the bashes
bash1a = "plink --bfile " + geno_path + " --indep-pairwise 1000 50 0.05"
bash1b = "plink --bfile " + geno_path + " --extract " + run_prefix + ".p_threshold_variants.tab" + " --indep-pairwise 1000 50 0.05"
bash2 = "plink --bfile " + geno_path + " --extract plink.prune.in --make-bed --out temp_genos"
bash3 = "plink --bfile temp_genos --recodeA --out " + run_prefix
bash4 = "cut -f 2,5 temp_genos.bim > " + run_prefix + ".variants_and_alleles.tab"
bash5 = "rm temp_genos.*"
bash6 = "rm " + run_prefix + ".raw"

# Set the bash command groups
cmds_a = [bash1a, bash2, bash3, bash4, bash5]
cmds_b = [bash1b, bash2, bash3, bash4, bash5]


if (gwas_path != "nope") & (geno_path != "nope"):
  p_thresh = args.p
  gwas_df_reduced = gwas_df[['SNP','p']]
  snps_to_keep = gwas_df_reduced.loc[(df['p'] <= p_thresh)]
  outfile = run_prefix + ".p_threshold_variants.tab"
  snps_to_keep.to_csv(outfile, index=False, sep = "\t")
  print("Your candidate variant list prior to pruning is right here", outfile, ".")

if (gwas_path == "nope") & (geno_path != "nope"):
  print("A list of pruned variants and the allele being counted in the dosages (usually the minor allele) can be found here ", run_prefix + ".variants_and_alleles.tab.")
  for cmd in cmds_a:
      subprocess.run(cmd, shell=True)

if (gwas_path != "nope") & (geno_path != "nope"):
  print("A list of pruned variants and the allele being counted in the dosages (usually the minor allele) can be found here", run_prefix + ".variants_and_alleles.tab.")
  for cmd in cmds_b:
      subprocess.run(cmd, shell=True)

if (geno_path != "nope"):
  raw_path = run_prefix + ".raw"
  raw_df = pd.read_csv(raw_path, engine = 'c', sep = " ")
  raw_df.drop(columns=['FID','MAT','PAT','SEX','PHENOTYPE'], inplace=True)
  raw_df.rename(columns={'IID':'ID'}, inplace=True)
  subprocess.run(bash6, shell=True)

"""--- 
<a id="4"></a>
## 4. Impute missing data
"""

impute_type = args.impute

if (geno_path != "nope"):
  if impute_type == 'mean': 
    raw_df = raw_df.fillna(raw_df.mean())
  if impute_type == 'median':
    raw_df = raw_df.fillna(raw_df.median())
  print("")
  print("You have just imputed your genotype features, covering up NAs with the column", impute_type, "so that analyses don't crash due to missing data.")
  print("Now your genotype features might look a little better (showing the first few lines of the left-most and right-most columns)...")
  print("#"*30)
  print(raw_df.describe())
  print("#"*30)
  print("")

if (addit_path != "nope"):
  if impute_type == 'mean': 
    addit_df = addit_df.fillna(addit_df.mean())
  if impute_type == 'median':
    addit_df = addit_df.fillna(addit_df.median())
  print("")
  print("You have just imputed your non-genotype features, covering up NAs with the column", impute_type, "so that analyses don't crash due to missing data.")
  print("Now your non-genotype features might look a little better (showing the first few lines of the left-most and right-most columns)...")
  print("#"*30)
  print(addit_df.describe())
  print("#"*30)
  print("")

"""--- 
<a id="5"></a>
## 5. VIF non-genotype features

Needs work, just overwriting addit_df with the VIF filtered addit_df as well as outputting a list of the features kept to CSV.  
CLI arguments of interest are args.vif and args.iter.

--- 
<a id="6"></a>
## 6. Scale continuous data

Parallelize for loop with numba after testing.
"""

if (addit_path != "nope"):

  cols = list(addit_df.columns)
  cols.remove('ID')
  addit_df[cols]

  for col in cols:
    if (addit_df[col].min() != 0) & (addit_df[col].max() != 1):
      addit_df[col] = (addit_df[col] - addit_df[col].mean())/addit_df[col].std(ddof=0)

  print("")
  print("You have just Z-scaled your non-genotype features, putting everything on a numeric scale similar to genotypes.")
  print("Now your non-genotype features might look a little closer to zero (showing the first few lines of the left-most and right-most columns)...")
  print("#"*30)
  print(addit_df.describe())
  print("#"*30)
  print("")

"""--- 
<a id="7"></a>
## 7. Export filtered subsets of data to HDF5
"""

outfile_h5 = run_prefix + ".dataForML.h5"

pheno_df.to_hdf(outfile_h5, key='pheno', mode = 'w')

if (geno_path != "nope"):
  raw_df.to_hdf(outfile_h5, key='geno')

if (addit_path != "nope"):
  addit_df.to_hdf(outfile_h5, key='addit')

"""--- 
<a id="8"></a>
## 8. Re-read from HDF5, merge and export merged dataset for analysis
"""

if (geno_path != "nope") & (addit_path != "nope"):

if (geno_path != "nope") & (addit_path == "nope"):

if (geno_path == "nope") & (addit_path != "nope"):

merged.to_hdf(outfile_h5, key='dataForML', mode = 'w')