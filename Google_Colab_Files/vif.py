# -*- coding: utf-8 -*-
"""VIF

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tfP7_bAYMEjqG2nEQQyJS6LoSFAOvzzN
"""



"""## VIF Calculation 
 - **Project:** For GenoMLv2
 - **Author(s):** Mary B. Makarious, Mike Nalls, and Gus Thomas 
 - **Date Notebook Created:** 23.08.2019
    - **Quick Description:** Redesigning VIF: Added preprocessing, new way to chunk and randomize dataframes prior to VIF, iteration function to rinse and repeat

---
### Quick Description: 
**Problem:** A lot of algorithms will not work when too many super correlated values are in the dataset

**Solution:** To use VIF to detect strong relations between 3+ more variables and take them out

### Motivation/Background:
**What is VIF?**

Variance Inflation Factor (VIF) is a measure of how much the variance of the coefficient derived from the model is inflated by co-linearity. 

It helps detect multicollinearity that you cannot catch just by eyeballing a pairwise correlation plot and even detects strong relations between 3 variables and more.

**How is it Calculated?**

It is calculated by taking the ratio of **the variance of all of the coefficients** divided by **the variance of that one variable’s coefficient when it is the only variable in the model**.
 - **VIF = 1:** No correlation between that predictor and the other variables
 - **VIF = 4:** Suspicious, needs to be looked into
 - **VIF = 5-10:** Look into it or drop the variable
 
**Why is multicollinearity an issue with regression?**
Well, the regression equation is the best fit line to represent the effects of your predictors and the dependant variable, and does not include the effects of one predictor on another.

Having high co-linearity (correlation of 1.00) between predictors will affect your coefficients and the accuracy, plus its ability to reduce the SSE (sum of squared errors — that thing you need to minimise with your regression)

**Why do this at all?**
While variables in a dataset are usually correlated to a small degree, highly collinear variables can be redundant in the sense that we only need to retain one of the features to give our model the necessary information.
Removing co-linear features is a method to reduce model complexity by decreasing the number of features and can help to increase model generalization

### Concerns/Impact on Related Code:
Current concerns with using this code and how it could likely impact outcomes?

### Thoughts for Future Development of Code in this Notebook:
How can this be better in the future?

---
## Proposed Workflow 
#### [0. Getting Started](#0)
This includes: 
 - Loading the necessary packages 
 - Reading in the data
 
#### [1. Preprocessing Data](#1)
This includes: 
 - Stripping any erroneous white spaces at the beginning or end of the column names
 - Dropping rows where there is at least one element missing
 - Dropping columns with non-numeric information
 - Sampling 100 rows at random to reduce memory overhead (change to 1K in final dataset)
 
#### [2. Generate Chunked, Randomized Dataframes](#2)
This includes: 
 - Dropping non-integer columns (like phenotype)
 - Generating a list from the column names
 - Randomizing the columns 
 - Generating dataframes from randomized columns
 - Returning a list of pd dfs that are cleaned, chunked, and randomized
 
#### [3. VIF Chunked Dataframes](#3)
This includes: 
 - This will be done on each chunk
 - Parallelize this?
 - Gluing the dataframe back together with no highly correlated variables 

#### [4. Iterate](#4)
This includes: 
 - Wash rinse repeat!


#### [5. Exporting Features to Remove](#5)
This includes: 
 - Following iterations, comparing the final VIF filtered df with the original and generating a list of features to remove

--- 
<a id="0"></a>
## 0. Getting Started
"""

# Import the necessary packages 
import sys
import argparse
import numpy as np
import pandas as pd
import math
import random
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from joblib import Parallel, delayed

"""### Reading in Data via GitHub"""

# Read in the data from GenoML's GitHub
discrete_testdata_url = "https://raw.githubusercontent.com/GenoML/genoml/master/genoml/test_data/discrete_example.dataForML"
discrete_df = pd.read_csv(discrete_testdata_url, sep="\t")

# Save out IDs to be used later 
IDs = discrete_df['ID']

# Peek the data
discrete_df.head()

"""--- 
<a id="1"></a>
## 1. Preprocessing the Data
"""

def check_df(df):
    """
    check_df takes in dataframe as an argument and strips it of missing values and non-numerical information.
    
    ### Arguments:
        df {pandas dataframe} -- A dataframe 
    
    ### Returns:
        cleaned_df {pandas dataframe} -- A cleaned dataframe with no NA values and only numerical values 
    """
    print("Stripping erroneous space, dropping non-numeric columns...") 
    df.columns = df.columns.str.strip()

    print("Drop any rows where at least one element is missing...")
    # Convert any infinite values to NaN prior to dropping NAs
    df.replace([np.inf, -np.inf], np.nan)
    df.dropna(how='any', inplace=True)
    
    print("Keeping only numerical columns...")
    int_cols = \
        df = df._get_numeric_data()
    
    print("Checking datatypes...")
    data_type = df.dtypes
    
    # Subset df to include only relevant numerical types
    int_cols = df.select_dtypes(include=["int", "int16", "int32", "int64", "float",
                                         "float16", "float32", "float64"]).shape[1]

    print("Sampling 100 rows at random to reduce memory overhead...")
    cleaned_df = df.sample(n=100).copy().reset_index()
    cleaned_df.drop(columns=["index"], inplace=True)
    
    print("Dropping columns that are not SNPs...")
    cleaned_df.drop(columns=['PHENO'], axis=1, inplace=True) 
    print("Dropped!")
    
    print("Cleaned!")
    return cleaned_df

#26/08: Changed to only remove PHENO - per Mike's comment

checked = check_df(discrete_df)
checked.head()

"""--- 
<a id="2"></a>
## 2. Generate Chunked, Randomized Dataframes
"""

# Create a function that takes in the column names, randomizes them, and spits out randomized dataframe
def randomize_chunks(cleaned_df, chunk_size=100):
    """
    randomize_chunks takes in a cleaned dataframe's column names, randomizes them, 
    and spits out randomized, chunked dataframes with only SNPs for the VIF calculation later
    
    ### Arguments:
        cleaned_df {pandas dataframe} -- A cleaned dataframe 
        chunk_size {int} -- Desired size of dataframe chunked (default=100)
        
    ### Returns:
        list_chunked_dfs {list dfs} -- A cleaned, randomized list of dataframes with only SNPs as columns
    """
    
    print("Shuffling columns...")
    col_names_list = cleaned_df.columns.values.tolist()
    col_names_shuffle = random.sample(col_names_list, len(col_names_list))
    cleaned_df = cleaned_df[col_names_shuffle]
    print("Shuffled!")
    
    print("Generating chunked, randomized dataframes...")
    chunked_list = [col_names_shuffle[i * chunk_size:(i + 1) * chunk_size] for i in range((len(col_names_shuffle) + chunk_size - 1) // chunk_size)] 
    df_list = []
    for each_list in chunked_list: 
        temp_df = cleaned_df[each_list].astype(float)
        df_list.append(temp_df.copy())
    
    no_chunks = len(df_list)
    print(f"The number of dataframes you have moving forward is {no_chunks}")
    print("Complete!")
    return df_list

randomized_dfs = randomize_chunks(checked)
#print(randomized_dfs[1])

"""--- 
<a id="3"></a>
## 3. VIF Chunked Dataframes
"""

def calculate_vif(df_list, threshold=5.0):
    """
    calculate_vif takes in an list of randomized dataframes and removes any variables
    that is greater than the specified threshold (default=5.0). This is to combat 
    multicolinearity between the variables. The function then returns a fully VIF-filtered
    dataframe.
    
    ### Arguments:
        df_list {list dfs} -- A list of cleaned, randomized pandas dataframes 
        threshold {float} -- Cut-off for dropping following the VIF calculation (default=5.0)

    ### Returns:
        glued_df {pandas df} -- A complete VIF-filtered dataframe 
    """
    dropped = True
    print(f"Dropping columns with a VIF threshold greater than {threshold}")
    
    for df in df_list:
        while dropped:
            # Loop until all variables in dataset have a VIF less than the threshold 
            variables = df.columns
            dropped = False
            vif = []

            # Changed to look at indexing 
            # Added simple joblib parallelization
            vif = Parallel(n_jobs=5)(delayed(variance_inflation_factor)(df[variables].values, df.columns.get_loc(var)) for var in variables) 
   
            max_vif = max(vif)
            
            if np.isinf(max_vif):
                maxloc = vif.index(max_vif)
                print(f'Dropping "{df.columns[maxloc]}" with VIF > {threshold}')
                dropped = True

            if max_vif > threshold:
                maxloc = vif.index(max_vif)
                print(f'Dropping "{df.columns[maxloc]}" with VIF = {max_vif:.2f}')
                df.drop([df.columns.tolist()[maxloc]], axis=1, inplace=True)
                dropped = True
                
    print("\nVIF calculation on all chunks complete! \n")
    
    print("Gluing the dataframe back together...")
    glued_df = pd.concat(df_list, axis=1)
    print("Full VIF-filtered dataframe generated!")
    
    return glued_df

vif_filtered_df = calculate_vif(randomized_dfs)
#vif_filtered_df.columns.values



"""--- 
<a id="4"></a>
## 4. Iterate
Wash-Rinse-Repeat
"""

def iterate(iterations=5):
    """
    The iterate function specifies a number of times to iterate through the shuffling 
    and VIF filtering functions 
    
    ### Arguments:
        number {int} -- An integer specifying the number of iterations to perform (default=5)

    ### Returns:
        features_toKeep {list} -- A list of features to keep, extracted from the final iteration
    """
    for iteration in range(iterations): 
        
        print(f"""
            \n\n
            Iteration {iteration+1}
            \n\n
            """)
        if iteration == 0: 
            step1 = randomize_chunks(checked, chunk_size=100)
            step2 = calculate_vif(step1, threshold=5.0)
        else:
            step3 = randomize_chunks(step2, chunk_size=100)
            step4 = calculate_vif(step3, threshold=5.0)
            step2 = step4

    # When done, make list of features to keep 
    features_toKeep = step4.columns.values.tolist()
    
    print(f"""
    \n\n
        Iteration Complete!
    \n\n
    """)
    return features_toKeep

features = iterate(2)

"""--- 
<a id="5"></a>
## 5. Exporting Features to Remove
This includes: 
 - Following iterations, comparing the final VIF filtered df with the original and generating a list of features to remove
"""

def toRemove(features_toKeep, cleaned_df):
    """
    The toRemove function specifies a list of features to be removed from the original dataframe
    following VIF calculation, because of their high correlation.  
    
    ### Arguments:
         features_toKeep {list} -- A list of features to keep, extracted from the final iteration
         cleaned_df {pandas df} -- The first dataframe that was cleaned, to extract features from

    ### Returns:
        features_toRemove {list} -- A list of features to remove from the original dataframe 
    """
    # Specify the features to keep that are VIF filtered
    toKeep = set(features)

    # List comprehension to generate a list of features to remove 
    # subtract toKeep from ORIGINAL to generate a list to remove 
    features_toRemove = [feature for feature in checked if feature not in toKeep]

    return(features_toRemove)

#27/08: Changed to export featurs to keep later on in GenoML - per Mike's comment

